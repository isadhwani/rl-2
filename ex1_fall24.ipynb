{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please install the following python libraries\n",
    "- python3: https://www.python.org/\n",
    "- numpy: https://numpy.org/install/\n",
    "- tqdm: https://github.com/tqdm/tqdm#installation\n",
    "- matplotlib: https://matplotlib.org/stable/users/installing/index.html\n",
    "- Starter Code was tested on Python 3.11.5\n",
    "\n",
    "If you encounter the error: \"IProgress not found. Please update jupyter & ipywidgets\"\n",
    "    \n",
    "Please install the ipywidgets as follows:\n",
    "\n",
    "    with pip, do\n",
    "    - pip install ipywidgets\n",
    "    \n",
    "    with conda, do\n",
    "    - conda install -c conda-forge ipywidgets\n",
    "    \n",
    "Restart your notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tqdm.notebook as tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Here is the implementation of the 10-armed Bandit problem/testbed. DO NOT CHANGE \n",
    "   Note that:\n",
    "       - call the reset function whenever you want to generate a new 10-armed Bandit problem\n",
    "\"\"\"\n",
    "class Bandit(object):\n",
    "    def __init__(self, k=10):\n",
    "        # Number of the actions\n",
    "        self.k = k\n",
    "\n",
    "        # Numpy array to store the true action value the k arms/actions\n",
    "        self.q_star = np.empty(self.k)\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the true action values to generate a new k-armed bandit problem\n",
    "        # Value for each arm is randomly sampled from a normal distribution \n",
    "        # with mean = 0, variance = 1.0. \n",
    "        self.q_star = np.random.normal(loc=0, scale=1, size=self.k)\n",
    "        \n",
    "    def best_action(self):\n",
    "        \"\"\"Return the indices of all best actions/arms in a list variable\n",
    "        \"\"\"\n",
    "        return np.where(self.q_star == self.q_star.max())[0].tolist()  \n",
    "\n",
    "    def step(self, act):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            act (int): index of the action\n",
    "        \"\"\"\n",
    "        # Compute the reward for each action\n",
    "        # The reward for each action at time step t is sampled from a Gaussian distribution\n",
    "        # For the k-th arm, the mean = q_star[k] (true value) and variance = 1\n",
    "        rewards = np.random.normal(loc=self.q_star, scale=np.ones(10), size=self.k)\n",
    "        return rewards[act]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Here is the plotting function you can directly use to plot the figures needed for Q5 and Q6\n",
    "\"\"\"\n",
    "\n",
    "# plot function\n",
    "def plot_curves(arr_list, legend_list, color_list, upper_bound, ylabel):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        arr_list (list): list of results arrays to plot\n",
    "        legend_list (list): list of legends corresponding to each result array\n",
    "        color_list (list): list of color corresponding to each result array\n",
    "        upper_bound (numpy array): array contains the best possible rewards for 2000 runs. the shape should be (2000,)\n",
    "        ylabel (string): label of the Y axis\n",
    "        \n",
    "        Note that, make sure the elements in the arr_list, legend_list and color_list are associated with each other correctly. \n",
    "        Do not forget to change the ylabel for different plots.\n",
    "        \n",
    "        To plot the upper bound for % Optimal action figure, set upper_bound = np.ones(num_step), where num_step is the number of steps.\n",
    "    \"\"\"\n",
    "    # set the figure type\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # PLEASE NOTE: Change the labels for different plots\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel(\"Steps\")\n",
    "    ax.set_ylim(-0.1, upper_bound.mean() + 0.1)\n",
    "    \n",
    "    # ploth results\n",
    "    h_list = []\n",
    "    for arr, legend, color in zip(arr_list, legend_list, color_list):\n",
    "        # compute the standard error\n",
    "        arr_err = arr.std(axis=0) / np.sqrt(arr.shape[0])\n",
    "        # plot the mean\n",
    "        h, = ax.plot(range(arr.shape[1]), arr.mean(axis=0), color=color, label=legend)\n",
    "        # plot the confidence band\n",
    "        arr_err = 1.96 * arr_err\n",
    "        ax.fill_between(range(arr.shape[1]), arr.mean(axis=0) - arr_err, arr.mean(axis=0) + arr_err, alpha=0.3, color=color)\n",
    "        # save the plot handle\n",
    "        h_list.append(h) \n",
    "    \n",
    "    # plot the upper bound\n",
    "    h = plt.axhline(y=upper_bound.mean(), color='k', linestyle='--', label=\"upper bound\")\n",
    "    h_list.append(h)\n",
    "    \n",
    "    # plot legends\n",
    "    ax.legend(handles=h_list)  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5: Implement the ε-greedy algorithm with incremental update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the scaffolding code for the epsilon-greedy agent. \n",
    "\n",
    "    1. Reset function: reset the Q value for each arm/action to be self.init. (e.g., self.init = 0)\n",
    "    \n",
    "    2. Choose action: select the arm/action using epsilon-greedy strategy.\n",
    "    \n",
    "    3. Update: update the time steps, Q values for k arms/actions and numbers of selecting each arm/action.\n",
    "    \n",
    "    4. argmax: find the indices of all maximal values in a numpu array.\n",
    "    \n",
    "Please finish the code under \"CODE HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyAgent(object):\n",
    "    def __init__(self, k: int, init: int, epsilon: float) -> None:\n",
    "        \"\"\"Epsilon greedy bandit agent\n",
    "\n",
    "        Args:\n",
    "            k (int): number of arms\n",
    "            init (init): initial value of Q-values\n",
    "            epsilon (float): random action probability\n",
    "        \"\"\"\n",
    "        # Number of the arms. For example, k = 10 for 10-armed Bandit problem\n",
    "        self.k = k\n",
    "\n",
    "        # Initial Q value\n",
    "        self.init = init\n",
    "\n",
    "        # Epsilon value\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # Q-values for each arm\n",
    "        self.Q = None\n",
    "        # Number of times each arm was pulled\n",
    "        self.N = None\n",
    "        # Current total number of steps\n",
    "        self.t = None\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Initialize or reset Q-values and counts\n",
    "\n",
    "        This method should be called after __init__() at least once\n",
    "        \"\"\"\n",
    "        self.Q = self.init * np.ones(self.k, dtype=np.float32)\n",
    "        self.N = np.zeros(self.k, dtype=int)\n",
    "        self.t = 0\n",
    "\n",
    "    def choose_action(self) -> int:\n",
    "        \"\"\"Choose which arm to pull\n",
    "\n",
    "        With probability 1 - epsilon, choose the best action (break ties arbitrarily, use argmax() from above).\n",
    "        \n",
    "        With probability epsilon, choose a random action.\n",
    "        \"\"\"\n",
    "        # CODE HERE: please implement the epsilon-greedy strategy to select the action\n",
    "        # return int \n",
    "\n",
    "    def update(self, action: int, reward: float) -> None:\n",
    "        \"\"\"Update Q-values and N after observing reward.\n",
    "\n",
    "        Args:\n",
    "            action (int): index of pulled arm\n",
    "            reward (float): reward obtained for pulling arm\n",
    "        \"\"\"\n",
    "        # increase the time step\n",
    "        self.t += 1\n",
    "        \n",
    "        # CODE HERE: implement the incremental update\n",
    "        # update the self.N\n",
    "        \n",
    "        # CODE HERE: update self.Q with the incremental update\n",
    "        # Note: please use the sample-average technique in equation 2.1\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def argmax(arr) -> int:\n",
    "        \"\"\"Argmax that breaks ties randomly\n",
    "\n",
    "        Takes in a list of values and returns the index of the item with the highest value, breaking ties randomly.\n",
    "\n",
    "        Note: np.argmax returns the first index that matches the maximum, so we define this method to use in EpsilonGreedy and UCB agents.\n",
    "        Args:\n",
    "            arr: sequence of values\n",
    "        \"\"\"\n",
    "        #CODE HERE: implement argmax_a Q(a) for the greedy action selection, breaking ties randomly.\n",
    "        # return int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Here is the function to run the epsilon greedy agent. Please complete the missing part under \"CODE HERE\"\n",
    "\"\"\"\n",
    "# run epsilon greedy \n",
    "def run_epsilon_greedy_agent(run_num, time_step, epsilon=0.0, init=0.0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        run_num (int): number of runs\n",
    "        time_step (int): number of time steps per run\n",
    "        epsilon (float): epsilon for the agent\n",
    "        init (float): initial value for the Q. (i.e., Q1)\n",
    "    \"\"\"\n",
    "    # DO NOT CHANGE: create the 10-armed Bandit problem\n",
    "    k = 10\n",
    "    env = Bandit(k)\n",
    "    env.reset()\n",
    "\n",
    "    # DO NOT CHANGE: create the agent with proper initial value and epsilon\n",
    "    agent = EpsilonGreedyAgent(k=k, init=init, epsilon=epsilon)\n",
    "    agent.reset()\n",
    "\n",
    "    # DO NOT CHANGE: create a numpy array to store rewards with shape (run_num, time_step)\n",
    "    # For example, results_rewards[r, t] stores the reward for step t in the r-th running trail\n",
    "    results_rewards = np.empty((run_num, time_step))\n",
    "    \n",
    "    # DO NOT CHANGE: create a numpy array to store optimal action proportion with shape (run_num, time_step)\n",
    "    # For example, results_action[r, t] stores 1 if the selected action at step t in the r-th runing trail is optimal\n",
    "    # and 0 otherwise.\n",
    "    results_action = np.empty((run_num, time_step))\n",
    "    \n",
    "    # DO NOT CHANGE: create a numpy array to save upper_bound (only for plotting rewards; it should be 1 for plotting action optimality proportion)\n",
    "    # For example, upper_bound[r] stores the true action value for the r-th running trail.\n",
    "    upper_bound = np.empty(run_num)\n",
    "    \n",
    "\n",
    "    # loop for trails starts\n",
    "    for r in tqdm.tqdm(range(run_num), desc=\"run number\", position=0):\n",
    "        \n",
    "        # CODE HERE: reset the environment to create a new 10-armed bandit problem.\n",
    "\n",
    "        # CODE HERE: reset the agent\n",
    "        \n",
    "        # CODE HERE: compute the upper bound for each running trial and update upper_bound[r]\n",
    "        \n",
    "        # loop for each trail a fixed number of steps\n",
    "        for t in tqdm.tqdm(range(time_step), desc=\"time step\", position=1, leave=False):\n",
    "            \n",
    "            # CODE HERE: get the best action to execute at step t \n",
    "            # act = int\n",
    "            \n",
    "            # CODE HERE: interact with the environment to receive rewards\n",
    "            # reward = float\n",
    "            \n",
    "            # Code HERE: update the agent based on the observed reward\n",
    "                     \n",
    "            \"\"\"DO NOT CHANGE BELOW\"\"\"\n",
    "            # save the reward\n",
    "            results_rewards[r, t] = reward\n",
    "            # check and save whether the action is optimal\n",
    "            if act in env.best_action():\n",
    "                results_action[r, t] = 1\n",
    "            else:\n",
    "                results_action[r, t] = 0\n",
    "            \n",
    "    return results_rewards, results_action, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Here is the implementation for running the experiment. You have to run the \"run_epsilon_greedy_agent\" function\n",
    "   for multiple times for different parameter combination. Please use smaller run_num and time_step for Debug only.\n",
    "   For example, run_num = 100, time_step = 100\n",
    "\"\"\"\n",
    "# always set the random seed for results reproduction\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "    \n",
    "# set the running parameters (Use 2000 runs and 1000 steps for final report)\n",
    "run_num = 2000\n",
    "time_step = 1000\n",
    "    \n",
    "# CODE HERE: \n",
    "# 1. run the epsilon-greedy agent experiment for initial value = 0.0, epsilon = 0.0\n",
    "# 2. run the epsilon-greedy agent experiment for initial value = 0.0, epsilon = 0.01\n",
    "# 3. run the epsilon-greedy agent experiment for initial value = 0.0, epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the \"Average reward\" figure\n",
    "plot_curves([rewards_array_for_first_method],\n",
    "            [\"curve legend name for first method\"],\n",
    "            [\"curve color for first method\"],\n",
    "            upper_bound_for_first_method,\n",
    "            \"y axis label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the \"% Optimal action\" figure\n",
    "plot_curves([action_array_for_first_method],\n",
    "            [\"curve legend name for first method\"],\n",
    "            [\"curve color for first method\"],\n",
    "            upper_bound_for_first_method, # should be 100%\n",
    "            \"y axis label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6: Implement the ε-greedy algorithm with optimistic initial values, and the bandit algorithm with UCB action selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Reproducing the Figure 2.3.\n",
    "Please note, instead of using the sample-average technique,\n",
    "Use equation 2.5 to update the Q values with \\alpha=0.1\n",
    "\"\"\"\n",
    "class EpsilonGreedyAgent(object):\n",
    "    def __init__(self, k: int, init: int, epsilon: float) -> None:\n",
    "        \"\"\"Epsilon greedy bandit agent\n",
    "\n",
    "        Args:\n",
    "            k (int): number of arms\n",
    "            init (init): initial value of Q-values\n",
    "            epsilon (float): random action probability\n",
    "        \"\"\"\n",
    "        # Number of the arms. For example, k = 10 for 10-armed Bandit problem\n",
    "        self.k = k\n",
    "\n",
    "        # Initial Q value\n",
    "        self.init = init\n",
    "\n",
    "        # Epsilon value\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # Q-values for each arm\n",
    "        self.Q = None\n",
    "        # Number of times each arm was pulled\n",
    "        self.N = None\n",
    "        # Current total number of steps\n",
    "        self.t = None\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Initialize or reset Q-values and counts\n",
    "\n",
    "        This method should be called after __init__() at least once\n",
    "        \"\"\"\n",
    "        self.Q = self.init * np.ones(self.k, dtype=np.float32)\n",
    "        self.N = np.zeros(self.k, dtype=int)\n",
    "        self.t = 0\n",
    "\n",
    "    def choose_action(self) -> int:\n",
    "        \"\"\"Choose which arm to pull\n",
    "\n",
    "        With probability 1 - epsilon, choose the best action (break ties arbitrarily, use argmax() from above).\n",
    "        \n",
    "        With probability epsilon, choose a random action.\n",
    "        \"\"\"\n",
    "        # CODE HERE: please implement the epsilon-greedy strategy to select the action\n",
    "        # return int\n",
    "\n",
    "    def update(self, action: int, reward: float) -> None:\n",
    "        \"\"\"Update Q-values and N after observing reward.\n",
    "\n",
    "        Args:\n",
    "            action (int): index of pulled arm\n",
    "            reward (float): reward obtained for pulling arm\n",
    "        \"\"\"\n",
    "        # increase the time step\n",
    "        self.t += 1\n",
    "        \n",
    "        # CODE HERE: implement the incremental update\n",
    "        # update the self.N\n",
    "        \n",
    "        # CODE HERE: update self.Q with the incremental update\n",
    "        # Note: For reproducing Figure 2.3, implement the exponential average (equation 2.5)\n",
    "        # Note: For reproducing Figure 2.4, implement the sample average (equation 2.1)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def argmax(arr) -> int:\n",
    "        \"\"\"Argmax that breaks ties randomly\n",
    "\n",
    "        Takes in a list of values and returns the index of the item with the highest value, breaking ties randomly.\n",
    "\n",
    "        Note: np.argmax returns the first index that matches the maximum, so we define this method to use in EpsilonGreedy and UCB agents.\n",
    "        Args:\n",
    "            arr: sequence of values\n",
    "        \"\"\"\n",
    "        #CODE HERE: implement argmax_a Q(a) for the greedy action selection, breaking ties randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Here is the implementation of the UCB agent. Please complete the missing part.\n",
    "\"\"\"\n",
    "class UCBAgent(object):\n",
    "    def __init__(self, k: int, init: int, c: float) -> None:\n",
    "        \"\"\"Epsilon greedy bandit agent\n",
    "\n",
    "        Args:\n",
    "            k (int): number of arms\n",
    "            init (init): initial value of Q-values\n",
    "            c (float): UCB constant that controls degree of exploration\n",
    "        \"\"\"\n",
    "        # Number of the arms. For example, k = 10 for 10-armed Bandit problem\n",
    "        self.k = k\n",
    "\n",
    "        # Initial Q value\n",
    "        self.init = init\n",
    "\n",
    "        # Epsilon value\n",
    "        self.c = c\n",
    "\n",
    "        # Q-values for each arm\n",
    "        self.Q = None\n",
    "        # Number of times each arm was pulled\n",
    "        self.N = None\n",
    "        # Current total number of steps\n",
    "        self.t = None\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Initialize or reset Q-values and counts\n",
    "\n",
    "        This method should be called after __init__() at least once\n",
    "        \"\"\"\n",
    "        self.Q = self.init * np.ones(self.k, dtype=np.float32)\n",
    "        self.N = np.zeros(self.k, dtype=int)\n",
    "        self.t = 0\n",
    "\n",
    "    def choose_action(self):\n",
    "        \"\"\"Choose which arm to pull\n",
    "\n",
    "        Use UCB action selection. Be sure to consider the case when N_t = 0 and break ties randomly (use argmax() from above)\n",
    "        \"\"\"\n",
    "        # CODE HERE: use UCB to select the action. Be sure to consider the case when N_t = 0\n",
    "        # and break ties randomly (use argmax() from above). The return should be an integer\n",
    "        # index of the action.\n",
    "        # return int\n",
    "\n",
    "\n",
    "    def update(self, action: int, reward: float) -> None:\n",
    "        \"\"\"Update Q-values and N after observing reward.\n",
    "\n",
    "        Args:\n",
    "            action (int): index of pulled arm\n",
    "            reward (float): reward obtained for pulling arm\n",
    "        \"\"\"\n",
    "        # increase the time step\n",
    "        self.t += 1\n",
    "\n",
    "        # CODE HERE: implement the incremental update\n",
    "        # update the self.N\n",
    "\n",
    "        # CODE HERER: update self.\n",
    "        # Note: For reproducing Figure 2.3, implement the exponential average (equation 2.5)\n",
    "        # Note: For reproducing Figure 2.4, implement the sample average (equation 2.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Here is the implementation of running the UCB agent. Please complete the missing part.\n",
    "\"\"\"\n",
    "# run epsilon greedy \n",
    "def run_ucb_agent(run_num, time_step, c):\n",
    "    # create the 10-armed Bandit problem\n",
    "    k = 10\n",
    "    env = Bandit(k)\n",
    "    env.reset()\n",
    "\n",
    "    # create the agent\n",
    "    my_agent = UCBAgent(k=k, init=0.0, c=c)\n",
    "    my_agent.reset()\n",
    "\n",
    "    # create a numpy array\n",
    "    results_rewards = np.empty((run_num, time_step))\n",
    "    \n",
    "    # create a numpy array\n",
    "    results_action = np.empty((run_num, time_step))\n",
    "\n",
    "    # loop starts\n",
    "    upper_bound = np.empty(run_num)\n",
    "    for r in tqdm.tqdm(range(run_num), desc=\"run number\", position=0):\n",
    "       \n",
    "        # CODE HERE: reset the environment and the agent\n",
    "        # create a new 10-armed bandit problem\n",
    "        \n",
    "        # CODE HERE: create a new agent\n",
    "\n",
    "        # CODE HERE: update upper_bound[r]\n",
    "\n",
    "        for t in tqdm.tqdm(range(time_step), desc=\"time step\", position=1, leave=False):\n",
    "            # CODE HERE: choose action for time step t\n",
    "            # act = int\n",
    "\n",
    "            # CODE HERE: interact with the environment\n",
    "            # reward = float\n",
    "\n",
    "            # CODE HERE: update the bandit agent with the observed reward\n",
    "\n",
    "            \n",
    "            # save the reward\n",
    "            results_rewards[r, t] = reward\n",
    "            # compute the optimality\n",
    "            if act in env.best_action():\n",
    "                results_action[r, t] = 1\n",
    "            else:\n",
    "                results_action[r, t] = 0\n",
    "            \n",
    "    return results_rewards, results_action, upper_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce Figure 2.3 using exponential average (equation 2.5 with alpha = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Here is the implementation for running the experiment. You have to run the \"run_ucb_agent\" function\n",
    "   for multiple times for different parameter combination. Please use smaller run_num and time_step for Debug only.\n",
    "   For example, run_num = 100, time_step = 1000\n",
    "\"\"\"\n",
    "# set the running parameters\n",
    "run_num = 2000 \n",
    "time_step = 1000\n",
    "    \n",
    "# CODE HERE: \n",
    "# 1. Run the epsilon-greedy agent experiment for initial value = 0.0, epsilon = 0.0\n",
    "# 2. Run the epsilon-greedy agent experiment for initial value = 5.0, epsilon = 0.0 \n",
    "# 3. Run the epsilon-greedy agent experiment for initial value = 0.0, epsilon = 0.1 \n",
    "# 4. Run the epsilon-greedy agent experiment for initial value = 5.0, epsilon = 0.1 \n",
    "# 5. Run the UCB agent experiment for c=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the \"% Optimal action\" figure\n",
    "plot_curves([action_array_for_first_method],\n",
    "            [\"curve legend name for first method\"],\n",
    "            [\"curve color for first method\"],\n",
    "            upper_bound_for_first_method, # should be 100%\n",
    "            \"y axis label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce Figure 2.4 using sample average (equation 2.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Here is the implementation for running the experiment. You have to run the \"run_ucb_agent\" function\n",
    "   for multiple times for different parameter combination. Please use smaller run_num and time_step for Debug only.\n",
    "   For example, run_num = 100, time_step = 1000\n",
    "\"\"\"\n",
    "# always set the random seed for results reproduction\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "\n",
    "# set the number of run\n",
    "run_num = 2000\n",
    "# set the number of time steps\n",
    "time_step = 1000\n",
    "\n",
    "# CODE HERE: \n",
    "# 1. Run the epsilon-greedy agent experiment for initial value = 0.0, epsilon = 0.0\n",
    "# 2. Run the epsilon-greedy agent experiment for initial value = 5.0, epsilon = 0.0 \n",
    "# 3. Run the epsilon-greedy agent experiment for initial value = 0.0, epsilon = 0.1 \n",
    "# 4. Run the epsilon-greedy agent experiment for initial value = 5.0, epsilon = 0.1 \n",
    "# 5. Run the UCB agent experiment for c=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the \"Average reward\" figure\n",
    "plot_curves([rewards_array_for_first_method],\n",
    "            [\"curve legend name for first method\"],\n",
    "            [\"curve color for first method\"],\n",
    "            upper_bound_for_first_method,\n",
    "            \"y axis label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7 Investigating nonstationary environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bandits(arms = 10):\n",
    "    # CODE HERE: Generate a k-armed bandit using the procedure described in Section 2.3\n",
    "    \n",
    "\n",
    "def generate_reward(bandits, arm):\n",
    "    # CODE HERE: Generate a random reward using the specified arm of the bandit, \n",
    "    # with reward distribution as described in Section 2.3\n",
    "\n",
    "    \n",
    "def gen_argmax(l, return_all = False):\n",
    "    # CODE HERE: Generalized argmax that finds all maximal elements and breaks ties \n",
    "    # If return_all is true, returns all maximal indices;\n",
    "    # otherwise, tie is broken randomly and some element is returned\n",
    "\n",
    "def plot_avg_se(data, num_se = 1.96, linestyle = 'k-'):\n",
    "    means = []\n",
    "    lowers = []\n",
    "    uppers = []\n",
    "    N = len(data)\n",
    "    T = len(data[0])\n",
    "    for t in range(T):\n",
    "        data_t = [d[t] for d in data]\n",
    "        mean = np.mean(data_t)\n",
    "        se = np.std(data_t) / np.sqrt(N)\n",
    "        means += [mean]\n",
    "        lowers += [mean - num_se * se]\n",
    "        uppers += [mean + num_se * se]\n",
    "    h, = plt.plot(range(1,T+1), means, linestyle)\n",
    "    plt.fill_between(range(1,T+1), lowers, uppers, color = linestyle[0], alpha = 0.2)\n",
    "    return h\n",
    "\n",
    "\n",
    "def q7(arms = 10, steps = 10000, trials = 2000, epsilon = 0.1, alpha = 0.1):\n",
    "    rewards = [[] for _ in range(2)]\n",
    "    optimals = [[] for _ in range(2)]\n",
    "    upper_bound = []\n",
    "    for trial in range(trials):\n",
    "        if (trial + 1) % 10 == 0:\n",
    "            print(trial + 1)\n",
    "            \n",
    "        # Initialize the bandit (all q* = 0)\n",
    "        # Implement epsilon-greedy in the loop;\n",
    "        # keep track of Q, N estimates and rewards, optimal actions, upper bounds\n",
    "        # for both alpha = None and alpha = 0.1\n",
    "        \n",
    "        # CODE HERE: Initilization        \n",
    "\n",
    "        \n",
    "        # CODE HERE: LOOP        \n",
    "        for t in range(steps):\n",
    "            # Hint: you should determine best action at time t and its value, do the Epsilon-greedy action selection            \n",
    "            # And explore both alpha = None and alpha = 0.1 at the same time \n",
    "            # Please use the variable name of: rs_none  rs_alpha to store the rewards and opts_none opts_alpha to store the optimal actions\n",
    "            # and use upper to represent the upper bound value.\n",
    "             \n",
    "            \n",
    "        # Store rewards and whether chosen actions were optimal\n",
    "        rewards[0] += [rs_none]\n",
    "        rewards[1] += [rs_alpha]\n",
    "        optimals[0] += [opts_none]\n",
    "        optimals[1] += [opts_alpha]\n",
    "        upper_bound += [upper]\n",
    "\n",
    "\n",
    "    # Plot average reward\n",
    "    plt.figure()\n",
    "    hs = []\n",
    "    hs += [plot_avg_se(rewards[0], linestyle = 'b-')]\n",
    "    hs += [plot_avg_se(rewards[1], linestyle = 'k-')]\n",
    "    hs += [plot_avg_se(upper_bound, linestyle = 'k--')]\n",
    "    plt.gca().set_xlim(0, steps)\n",
    "    plt.gca().set_ylim(0, 2)\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Average reward\")\n",
    "    plt.legend(hs, [\"Sample average\", r\"Exponential average, $\\alpha = 0.1$\", \"Upper bound\"])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Plot proportion of time optimal action was chosen\n",
    "    plt.figure()\n",
    "    hs = []\n",
    "    hs += [plot_avg_se(optimals[0], linestyle = 'b-')]\n",
    "    hs += [plot_avg_se(optimals[1], linestyle = 'k-')]\n",
    "    hs += [plt.hlines(1, 1, steps, linestyles = 'dashed')]\n",
    "    plt.gca().set_xlim(0, steps)\n",
    "    plt.gca().set_ylim(0, 1.05)\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Proportion optimal action\")\n",
    "    plt.legend(hs, [\"Sample average\", r\"Exponential average, $\\alpha = 0.1$\", \"Upper bound\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q7(steps = 10000, trials = 2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
